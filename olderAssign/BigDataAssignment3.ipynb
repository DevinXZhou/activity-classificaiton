{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data: Tools and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Xin Zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>In this assignment you are to use Scala and Spark. Turn in a Jupyter notebook that uses the Apache-Toree kernel. In problem 2-7 your code needs to use Spark and be able to run on a cluster using multiple worker nodes to the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>#1. Write a function that will put N doubles into a file. The doubles need to be normally distributed with mean 0 and standard deviation 1. The function should have two arguments: N and the full name of the file (ie includes path to file location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5713469585202726 0.64984617103597 1.054864833984803 0.3685464527890387 -0.8646884832901189 -0.5215683912930904 -0.20124766664715338 -0.21946351319069785 -0.11485764505829102 -0.3091966918688628"
     ]
    }
   ],
   "source": [
    "import java.io._\n",
    "import scala.io.Source._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "def NDFileGenerate(N:Int, fileDir: String, parsingString: String){\n",
    "    val R = new scala.util.Random\n",
    "    val values = Array.fill(N){R.nextGaussian()}\n",
    "    val pw = new PrintWriter(new File(fileDir))\n",
    "    pw.write(values.mkString(parsingString))\n",
    "    pw.close\n",
    "}\n",
    "//Test for NDFileGenerate function\n",
    "NDFileGenerate(10,\"doubles.txt\", \" \")\n",
    "val lines = fromFile(\"doubles.txt\").getLines\n",
    "var L:String = \"\"\n",
    "for (line <- lines){\n",
    "    L = line\n",
    "    print (line)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Create a file with 50,000 doubles using the function from problem 1. This file will be used for the next several problems. It is best if you put the file in the current directory to avoid paths that do not exist on other machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NDFileGenerate(50000,\"doubles.txt\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>#2.Read the file created in #1 into an RDD and compute the mean and standard deviation of the doubles in the file. Work on the RDD, that is do not convert the RDD to a DataFrame or Dataset.. You are to use Spark code to compute the values as we want this to run on a cluster using multiple machines. So the pure Scala code you used in assignment will not work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean value calculated in RDD is 1.7269809673229486E-4\n",
      "The standard deviation calculated in RDD is 1.001931736052103\n"
     ]
    }
   ],
   "source": [
    "val myRDD = sc.textFile(\"doubles.txt\").flatMap(line => line.split(\" \").map(_.toDouble))\n",
    "println(\"The mean value calculated in RDD is \" + myRDD.mean())\n",
    "println(\"The standard deviation calculated in RDD is \" + myRDD.stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=green>#3. Repeat #2 but using a DataFrame instead of RDD. Here work on the DataFrame not an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*SerializeFromObject [input[0, double, false] AS value#1288]\n",
      "+- Scan ExternalRDDScan[obj#1287]\n",
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "| 0.7588979840782909|\n",
      "|-1.5014978846757265|\n",
      "|  1.281998201953683|\n",
      "|-0.4095865027394471|\n",
      "|-0.5229824261995106|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+------------------+\n",
      "|               Mean|Standard Deviation|\n",
      "+-------------------+------------------+\n",
      "|1.72698096732263E-4|1.0019417555197556|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//If I directly use the file which contains \" \" space as parsing, then use spark.read.text()\n",
    "//It will become one column which contains one string, parsing a string inside a DF column is complicated and unsolved\n",
    "//convert RDD to DF instead of creating dataframe from text file\n",
    "import org.apache.spark.sql.functions._\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "val df = myRDD.toDF()\n",
    "df.explain() //successfully created data frame form rdd if explain() can be used\n",
    "df.show(5)\n",
    "val mean_stddev = df.agg(\n",
    "    mean(\"value\").alias(\"Mean\"),\n",
    "    stddev(\"value\").alias(\"Standard Deviation\")\n",
    ")\n",
    "mean_stddev.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>#4. Using a Data Frame create a random sample of about 100 elements of the file created in #2 and compute the mean of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size is 100\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|  -1.328019774772998|\n",
      "|  0.5504744327125294|\n",
      "|-0.34573308691858545|\n",
      "|-0.27494027036026436|\n",
      "| -1.1173925220264196|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val withReplacement = false\n",
    "val sample = df.sample(withReplacement, 0.005).limit(100)\n",
    "println(\"sample size is \"+ sample.count())\n",
    "sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>#5. Create a file of 100 normally distributed doubles. Read the doubles from the file into an RDD. Using the RDD create a sliding window of size 20 and compute the mean of each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of means is 81\n",
      "Five samples from means: \n",
      "-0.050513469699788226\n",
      "-0.013737053566370139\n",
      "0.016241509018171025\n",
      "0.0684814896626845\n",
      "0.005043026330456923\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.rdd.RDDFunctions\n",
    "NDFileGenerate(100,\"doubles100.txt\", \" \")\n",
    "val rdd100 = sc.textFile(\"doubles100.txt\").flatMap(line => line.split(\" \").map(_.toDouble))\n",
    "val rddFS = RDDFunctions.fromRDD(rdd100)\n",
    "val means = rddFS.sliding(20).map{values20 =>\n",
    "    val sum = values20.reduce(_ + _)\n",
    "    sum/20\n",
    "}\n",
    "println(\"The size of means is \" + means.collect.size)\n",
    "println(\"Five samples from means: \")\n",
    "means.take(5).foreach(mean => println(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>#6. The file “multiple-sites.tsv” contains two columns: site and dwell-time. Using Spark compute the average dwell time for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of average of sites is: 20\n",
      "+----+-----------------+\n",
      "|site|          Average|\n",
      "+----+-----------------+\n",
      "|   0|79.85106382978724|\n",
      "|   1|            106.0|\n",
      "|   2|88.22916666666667|\n",
      "|   3|97.47916666666667|\n",
      "|   4|94.33333333333333|\n",
      "+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val dwellDir = \"Assignment3Data/multiple-sites.tsv\"\n",
    "val reader = spark.read \n",
    "reader.option(\"header\", true).option(\"inferSchema\",true).option(\"sep\", \"\\t\")\n",
    "val dwellDf = reader.csv(dwellDir) \n",
    "// dwellDf.printSchema\n",
    "// dwellDf.show(3)\n",
    "val average = dwellDf.groupBy(\"site\").agg(mean(\"dwell-time\").alias(\"Average\"))\n",
    "println(\"The size of average of sites is: \" + average.count())\n",
    "average.sort(\"site\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>#7. The file “multiple-sites.tsv” contains two columns: date and dwell-time. Using Spark compute the following:<br>\n",
    "1. The average dwell time each hour<br>\n",
    "2. The average dwell time per day of week<br>\n",
    "3. The average dwell time on week-days(Monday-Friday)<br>\n",
    "4. Average dwell time on the weekend.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----+---------+\n",
      "|          TimeStamp|dwell-time|      Date|Hour|DayOfWeek|\n",
      "+-------------------+----------+----------+----+---------+\n",
      "|2014-12-31 16:03:43|        74|2014-12-31|  16|Wednesday|\n",
      "|2014-12-31 16:32:12|       109|2014-12-31|  16|Wednesday|\n",
      "+-------------------+----------+----------+----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "//transform the dataframes\n",
    "val reader = spark.read \n",
    "reader.option(\"header\", true).option(\"inferSchema\",true).option(\"sep\", \"\\t\")\n",
    "val dwellDf = reader.csv(\"Assignment3Data/dwell-times.tsv\") \n",
    "\n",
    "val df = dwellDf.withColumnRenamed(\"date\", \"TimeStamp\").\n",
    "withColumn(\"Date\", to_date(col(\"TimeStamp\"))).\n",
    "withColumn(\"Hour\", hour(col(\"TimeStamp\"))).\n",
    "withColumn(\"DayOfWeek\", date_format(col(\"Date\"),\"EEEE\"))\n",
    "df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 groups of average dwell time per hour\n",
      "+----+-------------------+\n",
      "|Hour|AvgDwellTimePerHour|\n",
      "+----+-------------------+\n",
      "|   0|    94.708670095518|\n",
      "|   1|  92.20954287620954|\n",
      "|   2|  96.85937970490816|\n",
      "|   3|  92.57110924839341|\n",
      "|   4|  91.33086825527253|\n",
      "+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 7 groups of average dwell time for per day of week\n",
      "+---------+----------------------+                                              \n",
      "|DayOfWeek|AvgDwellTimeDayPerWeek|\n",
      "+---------+----------------------+\n",
      "|   Monday|     89.11023872679046|\n",
      "|  Tuesday|     89.73922367574522|\n",
      "|   Friday|     90.44266729389628|\n",
      "| Thursday|      91.4947995556902|\n",
      "|Wednesday|     91.57283771155643|\n",
      "|   Sunday|    106.49005681818181|\n",
      "| Saturday|    116.88253012048193|\n",
      "+---------+----------------------+\n",
      "\n",
      "The average dwell time on week-days(Monday-Friday) shown below:\n",
      "+----------------+\n",
      "| AvgTimeWeekdays|\n",
      "+----------------+\n",
      "|90.4719533927357|\n",
      "+----------------+\n",
      "\n",
      "Average dwell time on the weekend shown below:\n",
      "+------------------+\n",
      "|   AvgTimeWeekends|\n",
      "+------------------+\n",
      "|111.68629346933187|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// 7.1 The average dwell time each hour\n",
    "val hourAvg = df.groupBy(\"Hour\").agg(mean(\"dwell-time\").alias(\"AvgDwellTimePerHour\"))\n",
    "println(\"There are \" + hourAvg.count + \" groups of average dwell time per hour\")\n",
    "hourAvg.sort(\"Hour\").show(5)\n",
    "\n",
    "//7.2 The average dwell time per day of week\n",
    "val dayOfWeek = df.groupBy(\"DayOfWeek\").agg(mean(\"dwell-time\").alias(\"AvgDwellTimeDayPerWeek\"))\n",
    "println(\"There are \" + dayOfWeek.count + \" groups of average dwell time for per day of week\")\n",
    "dayOfWeek.sort(\"AvgDwellTimeDayPerWeek\").show\n",
    "\n",
    "//7.3 The average dwell time on week-days(Monday-Friday)\n",
    "println(\"The average dwell time on week-days(Monday-Friday) shown below:\")\n",
    "val weekdays = dayOfWeek.where(col(\"DayOfWeek\") =!= \"Saturday\").where(col(\"DayOfWeek\") =!= \"Sunday\")\n",
    "val weekdayAvg = weekdays.agg(mean(\"AvgDwellTimeDayPerWeek\").alias(\"AvgTimeWeekdays\"))\n",
    "weekdayAvg.show\n",
    "\n",
    "//7.4 Average dwell time on the weekend.\n",
    "println(\"Average dwell time on the weekend shown below:\")\n",
    "val weekends = dayOfWeek.where(col(\"DayOfWeek\").contains(\"Saturday\").or(col(\"DayOfWeek\").contains(\"Sunday\")))\n",
    "val weekendAvg = weekends.agg(mean(\"AvgDwellTimeDayPerWeek\").alias(\"AvgTimeWeekends\"))\n",
    "weekendAvg.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=green>#8. Do the average dwell times computed in #7 indicate any difference in users behavior?<br>\n",
    "<font color=black>Users spends more time during weekend, and spend less time during weekdays. If we rank the average dwell time, we can see that the Monday has the least average dwell time, and Sunday Saturday has the most average dwell time. Moreover, Friday ranks at third place which makes sense because some people can spend a little more time on Friday night, but most other people prefer hang out and spend less time on internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

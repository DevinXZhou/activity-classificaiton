{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|predict|  a|  b|  c|  d|  e|  f|  g|  h|  i|  j|  k|  l|  m|  n|  o|  q|  r|  s|  t|  u|  v|\n",
      "+-------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|      p|  x|  s|  n|  t|  p|  f|  c|  n|  k|  e|  e|  s|  s|  w|  w|  w|  o|  p|  k|  s|  u|\n",
      "|      e|  x|  s|  y|  t|  a|  f|  c|  b|  k|  e|  c|  s|  s|  w|  w|  w|  o|  p|  n|  n|  g|\n",
      "|      e|  b|  s|  w|  t|  l|  f|  c|  b|  n|  e|  c|  s|  s|  w|  w|  w|  o|  p|  n|  n|  m|\n",
      "|      p|  x|  y|  w|  t|  p|  f|  c|  n|  n|  e|  e|  s|  s|  w|  w|  w|  o|  p|  k|  s|  u|\n",
      "|      e|  x|  s|  g|  f|  n|  f|  w|  b|  k|  t|  e|  s|  s|  w|  w|  w|  o|  e|  n|  a|  g|\n",
      "+-------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "val html = scala.io.Source.fromURL(\"http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\").mkString\n",
    "val list = html.split(\"\\n\").map{line => line.split(\",\")}.toSeq\n",
    "val oldDF = list.toDF()\n",
    "//val rdds = sc.parallelize(list)\n",
    "\n",
    "val dF = oldDF.select(\n",
    "  $\"value\".getItem(0).as(\"predict\"),\n",
    "  $\"value\".getItem(1).as(\"a\"),\n",
    "  $\"value\".getItem(2).as(\"b\"),\n",
    "  $\"value\".getItem(3).as(\"c\"),\n",
    "  $\"value\".getItem(4).as(\"d\"),\n",
    "  $\"value\".getItem(5).as(\"e\"),\n",
    "  $\"value\".getItem(6).as(\"f\"),\n",
    "  $\"value\".getItem(7).as(\"g\"),\n",
    "  $\"value\".getItem(8).as(\"h\"),\n",
    "  $\"value\".getItem(9).as(\"i\"),\n",
    "  $\"value\".getItem(10).as(\"j\"),\n",
    "  $\"value\".getItem(11).as(\"k\"),\n",
    "  $\"value\".getItem(12).as(\"l\"),\n",
    "  $\"value\".getItem(13).as(\"m\"),\n",
    "  $\"value\".getItem(14).as(\"n\"),\n",
    "  $\"value\".getItem(15).as(\"o\"),\n",
    "  $\"value\".getItem(16).as(\"p\"),\n",
    "  $\"value\".getItem(17).as(\"q\"),\n",
    "  $\"value\".getItem(18).as(\"r\"),\n",
    "  $\"value\".getItem(19).as(\"s\"),\n",
    "  $\"value\".getItem(20).as(\"t\"),\n",
    "  $\"value\".getItem(21).as(\"u\"),\n",
    "  $\"value\".getItem(22).as(\"v\")).drop(\"value\").drop(\"p\")\n",
    "dF.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.RFormula\n",
    "val formula = new RFormula().setFormula(\"predict ~ a+b+c+d+e+f+g+h+i+j+k+l+m+n+o+q+r+s+t+u+v\")\n",
    "val preparedDF = formula.fit(dF).transform(dF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|          prediction|label|            features|\n",
      "+--------------------+-----+--------------------+\n",
      "|0.005355146200169348|  0.0|(95,[3,6,12,22,26...|\n",
      "|0.005355146200169348|  0.0|(95,[3,6,12,22,26...|\n",
      "|0.005355146200169348|  0.0|(95,[3,6,12,22,26...|\n",
      "|0.005355146200169348|  0.0|(95,[3,6,12,22,26...|\n",
      "|0.005355146200169348|  0.0|(95,[3,6,12,22,26...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.06261856603668746\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "\n",
    "\n",
    "val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(12).fit(preparedDF)\n",
    "val Array(trainingData, testData) = preparedDF.randomSplit(Array(0.7, 0.3))\n",
    "val rf = new RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "val pipeline = new Pipeline().setStages(Array(featureIndexer, rf))\n",
    "\n",
    "// Train model. This also runs the indexer.\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 2382\n",
      "True positive: 1179\n",
      "TRue negative: 1201\n",
      "False positive: 0\n",
      "False negative: 2\n",
      "Misclassification rate: 0.084%\n"
     ]
    }
   ],
   "source": [
    "val predictedLabel = predictions.\n",
    "        select(\"label\", \"prediction\").\n",
    "        withColumn(\"prediction\", \n",
    "        when(col(\"prediction\") > 0.5, 1).otherwise(0))\n",
    "val TP = predictedLabel.filter(\"label=1\").filter(\"prediction=1\").count()\n",
    "val TN = predictedLabel.filter(\"label=0\").filter(\"prediction=0\").count()\n",
    "val FP = predictedLabel.filter(\"label=0\").filter(\"prediction=1\").count()\n",
    "val FN = predictedLabel.filter(\"label=1\").filter(\"prediction=0\").count()\n",
    "val total = predictedLabel.count()\n",
    "println(\"Total test samples: \"+ total)\n",
    "println(\"True positive: \"+ TP)\n",
    "println(\"TRue negative: \"+ TN)\n",
    "println(\"False positive: \"+ FP)\n",
    "println(\"False negative: \"+ FN)\n",
    "println(\"Misclassification rate: \" + math.round((FP+FN)*100000.0/total.toDouble)/1000.0+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mushroomModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io._\n",
    "val pw = new PrintWriter(new File(\"mushroomConfusionMetrics.txt\" ))\n",
    "val metrics = Array(TP, TN, FP, FN)\n",
    "pw.write(\"TP, TN, FP, FN \\n\")\n",
    "pw.write(metrics.mkString(\" \"))\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
